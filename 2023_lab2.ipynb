{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnson6314/NTHU_2023_DLBOI_HW/blob/main/2023_lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li0bVCTuxc6n"
      },
      "source": [
        "# Lab 2: Build an ANN for Chest X-ray Image Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjQl7EGxxc6o"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Pneumonia is a prevalent lung infection, and its early detection is paramount to ensure prompt and suitable treatment. In this tutorial, we'll leverage PyTorch, a leading deep learning framework, to construct a classifier using a chest X-ray dataset that distinguishes whether a patient is diagnosed with pneumonia. This dataset encompasses two distinct classes, with each class housing 1,000 grayscale images.\n",
        "\n",
        "By the culmination of this tutorial, you'll have the prowess to:\n",
        "- Preprocess and load the dataset.\n",
        "- Build and train a classifier utilizing PyTorch.\n",
        "- Evaluate your developed model.\n",
        "\n",
        "For a deeper dive into PyTorch and its functionalities, consider exploring the [official PyTorch website](https://pytorch.org/).\n",
        "\n",
        "### References\n",
        "\n",
        "- [Chest X-ray Pneumonia Dataset from Kaggle](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia?resource=download)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hWNQ-5P72m-"
      },
      "outputs": [],
      "source": [
        "# Check your GPU status.\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGJcRwdzM7pS"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/week6/normal.npy\n",
        "!wget https://raw.githubusercontent.com/TacoXDD/homeworks/master/week6/pneumonia.npy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-1PsC--M7pT"
      },
      "source": [
        "## A. Data Loading and Preprocessing\n",
        "\n",
        "In this section, we'll go through the steps to load and preprocess the Chest X-ray dataset:\n",
        "\n",
        "1. **Import Necessary Libraries**: We begin by importing the required Python modules such as `torch` and `numpy`.\n",
        "\n",
        "2. **Load the Dataset**: The datasets for abnormal (indicative of pneumonia) and normal scans are loaded from their respective `.npy` files.\n",
        "\n",
        "3. **Label Assignment**: For the purpose of binary classification:\n",
        "   - Scans indicating the presence of pneumonia are labeled as `1`.\n",
        "   - Scans that are normal are labeled as `0`.\n",
        "\n",
        "4. **Data Splitting**: The dataset is then divided into training and validation sets, using a predefined `split_point` of 800.\n",
        "\n",
        "5. **Conversion to PyTorch Tensors**: The numpy arrays of the images and labels are converted to PyTorch tensors to facilitate GPU-accelerated computations in the next steps.\n",
        "\n",
        "6. **Dataset and DataLoader Creation**: Using PyTorch's `TensorDataset` and `DataLoader` utilities, we create datasets and data loaders for both training and validation data. This aids in easy batch processing and shuffling of the data during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oScQ0GG6xc6r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Load Dataset\n",
        "abnormal_scans = np.load('pneumonia.npy')\n",
        "normal_scans = np.load('normal.npy')\n",
        "\n",
        "print(f'Shape of abnormal_scans: {abnormal_scans.shape}')\n",
        "print(f'Shape of normal_scans: {normal_scans.shape}')\n",
        "\n",
        "# For the data having presence of pneumonia\n",
        "# assign 1, for the normal ones assign 0.\n",
        "abnormal_labels = np.array([1 for _ in range(len(abnormal_scans))])\n",
        "normal_labels = np.array([0 for _ in range(len(normal_scans))])\n",
        "\n",
        "split_point = 800\n",
        "\n",
        "x_train = np.concatenate((abnormal_scans[:split_point], normal_scans[:split_point]), axis=0)\n",
        "y_train = np.concatenate((abnormal_labels[:split_point], normal_labels[:split_point]), axis=0)\n",
        "x_val = np.concatenate((abnormal_scans[split_point:], normal_scans[split_point:]), axis=0)\n",
        "y_val = np.concatenate((abnormal_labels[split_point:], normal_labels[split_point:]), axis=0)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x_train = torch.from_numpy(x_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "x_val = torch.from_numpy(x_val).float()\n",
        "y_val = torch.from_numpy(y_val).long()\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TensorDataset(x_train, y_train)\n",
        "val_dataset = TensorDataset(x_val, y_val)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f'Number of samples in train and validation are {len(train_loader.dataset)} and {len(val_loader.dataset)}.')\n",
        "print(f'X_train: max value is {x_train.max().item()}, min value is {x_train.min().item()}, data type is {x_train.dtype}.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_0KQsRbl9zB"
      },
      "outputs": [],
      "source": [
        "# Check images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "for images, labels in train_loader:\n",
        "    print(\"Size of the image is:\", images[0].shape)\n",
        "    for i in range(8):\n",
        "        ax = plt.subplot(2, 4, i + 1)\n",
        "        plt.title(f\"{'abnormal' if labels[i] else 'normal'}\")\n",
        "        plt.axis('off')\n",
        "        plt.imshow(images[i], cmap='gray')\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaLGtT28xc6s"
      },
      "source": [
        "## B. Defining Neural Networks in PyTorch\n",
        "\n",
        "PyTorch offers the foundational module `torch.nn.Module` for designing neural networks. When it comes to defining a custom neural network, there are two primary approaches:\n",
        "\n",
        "1. **Sequential Method**: Design your neural network as an ordered sequence of layers.\n",
        "2. **Custom Class Method**: By extending `torch.nn.Module`, you can craft a network with more complex architectures and specific functionalities.\n",
        "\n",
        "While the Sequential method is apt for standard networks where layers are stacked in order, the Custom Class method offers greater flexibility, allowing you to create intricate architectures.\n",
        "\n",
        "### Common Layers in PyTorch:\n",
        "\n",
        "* `Linear`: This represents a densely connected linear layer, similar to a single-layer perceptron.\n",
        "* Activation Layers such as `Softmax`, `Sigmoid`, and `ReLU`: These layers introduce the needed non-linearity into the model.\n",
        "* Additionally, PyTorch provides specialized layers tailored for specific architectures, including convolutional and recurrent layers, which we'll explore later in this course.\n",
        "\n",
        "> **Note**: A unique feature of PyTorch is that many activation and loss functions are available in two forms: as **functions** (found in the `torch.nn.functional` namespace) and **as layers** (located in the `torch.nn` namespace). For activations, it's often more convenient to use the functional components from `torch.nn.functional` rather than instantiate separate layer objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZP0t5kOxc6s"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Model definition\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(256*256*1, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 1)\n",
        ").cuda()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvLTU-IfZLqn"
      },
      "source": [
        "## C. Training the Neural Network\n",
        "\n",
        "In this section, we'll walk through the process of training and validating our neural network on the chest X-ray dataset. Here's a step-by-step breakdown:\n",
        "\n",
        "1. **Setting Hyperparameters and Initialization**:\n",
        "   - We'll be training our model for `30` epochs.\n",
        "   - We initialize the `best_val_loss` as `infinity` to keep track of our best validation loss during training.\n",
        "\n",
        "2. **Loss and Optimizer**:\n",
        "   - We employ the `BCEWithLogitsLoss`, which is apt for binary classification tasks. This loss combines a sigmoid layer and the binary cross-entropy loss, making it more numerically stable than using them separately.\n",
        "   - The optimizer of choice is `Adam`, with a learning rate of `1e-3`.\n",
        "   - We'll also use a learning rate scheduler, `CosineAnnealingLR`, which adjusts the learning rate using a cosine annealing schedule.\n",
        "\n",
        "3. **Training Loop**:\n",
        "   - For each epoch, we put our model into training mode with `model.train()`.\n",
        "   - We iterate over batches of data from our `train_loader`, sending the data to the GPU (with `.cuda()`), normalizing our images, and performing the forward and backward passes.\n",
        "   - The gradients are then used to update the model's weights.\n",
        "\n",
        "4. **Validation Loop**:\n",
        "   - After training, we switch our model to evaluation mode using `model.eval()`.\n",
        "   - We then evaluate our model's performance on the validation set. This involves forwarding our data through the model and computing the loss and accuracy.\n",
        "   - The predictions are obtained by applying a threshold of `0.5` to the output sigmoid probabilities.\n",
        "\n",
        "5. **Learning Rate Adjustment**:\n",
        "   - After each epoch, we adjust the learning rate using our scheduler.\n",
        "\n",
        "6. **Model Checkpointing**:\n",
        "   - If our current model achieves a lower validation loss than our best recorded loss, we save this model as a checkpoint.\n",
        "\n",
        "By the end of this training loop, we aim to have a trained model that performs well on our validation dataset, and we save the best-performing model to `model_classification.pth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45ol4lpVxc6t"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "epochs = 30\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Criterion and Optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "lr_scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader)*epochs, eta_min=0)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    train_correct = 0\n",
        "    total_train_samples = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.cuda()\n",
        "        images = images / 255.\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        labels = labels.float().unsqueeze(1)  # Convert labels to float and match shape with outputs\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate correct predictions for training data\n",
        "        train_predicted = torch.sigmoid(outputs) > 0.5\n",
        "        train_correct += (train_predicted.float() == labels).sum().item()\n",
        "        total_train_samples += labels.size(0)\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_accuracy = 100. * train_correct / total_train_samples\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.cuda()\n",
        "            images = images / 255.\n",
        "            labels = labels.cuda()\n",
        "            outputs = model(images)\n",
        "            labels_float = labels.float().unsqueeze(1)  # Convert labels to float and match shape with outputs\n",
        "            loss = criterion(outputs, labels_float)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            predicted = torch.sigmoid(outputs) > 0.5\n",
        "            correct += (predicted.float() == labels_float).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = 100. * correct / total\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "    # Learning rate update\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Checkpoint\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        torch.save(model.state_dict(), 'model_classification.pth')\n",
        "\n",
        "    # Store performance\n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_losses.append(avg_val_loss)\n",
        "    val_accuracies.append(val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjmYxAJnxc6t"
      },
      "source": [
        "### Visualizing model performance\n",
        "\n",
        "Here the model accuracy and loss for the training and the validation sets are plotted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHpS0Q7vxc6t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plotting training and validation accuracy\n",
        "ax[0].plot(train_accuracies)\n",
        "ax[0].plot(val_accuracies)\n",
        "ax[0].set_title(\"Model Accuracy\")\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "ax[0].set_ylabel(\"Accuracy\")\n",
        "ax[0].legend([\"Train\", \"Val\"])\n",
        "\n",
        "# Plotting training and validation loss\n",
        "ax[1].plot(train_losses)\n",
        "ax[1].plot(val_losses)\n",
        "ax[1].set_title(\"Model Loss\")\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "ax[1].set_ylabel(\"Loss\")\n",
        "ax[1].legend([\"Train\", \"Val\"])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSbQfz6ixc6t"
      },
      "source": [
        "## D. Model Evaluation with a Test Image\n",
        "\n",
        "In this section, we'll evaluate our trained model using a random test image from the validation set to understand its prediction capability.\n",
        "\n",
        "1. **Loading the Best Model**: We begin by loading the best model saved during training using the `load_state_dict` function.\n",
        "\n",
        "2. **Random Image Selection**: A random index is chosen from the validation dataset, which will be used to fetch the corresponding X-ray image.\n",
        "\n",
        "3. **Image Normalization**: Just like during the training process, we normalize our test image to ensure consistent input data distribution for the model.\n",
        "\n",
        "4. **Prediction**: We pass the normalized test image through the model to obtain its prediction. Since the output of our model is in logits, we apply the `sigmoid` function to convert these logits to probabilities for our two classes - \"normal\" and \"abnormal\".\n",
        "\n",
        "5. **Output Interpretation**:\n",
        "We determine the model's final prediction by selecting the class with the highest confidence score.\n",
        "\n",
        "6. **Visualization**: Lastly, we display the test image using `imshow` to have a visual understanding of what the model sees. The ground truth label for this image is also shown in the title for reference.\n",
        "\n",
        "By repeatedly executing this section, you can evaluate the model's performance on different test images from the validation set and gain insight into its strengths and potential areas of improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tmDjE5hxc6t"
      },
      "outputs": [],
      "source": [
        "# Load best weights\n",
        "model.load_state_dict(torch.load('model_classification.pth'))\n",
        "model.eval()  # set the model to evaluation mode\n",
        "\n",
        "index = np.random.randint(0, len(x_val))\n",
        "print(f'Take # {index} as test image.')\n",
        "\n",
        "# Normalize the image just like during training\n",
        "test_image = (x_val[index] / 255.0).clone().detach().float().cuda()\n",
        "test_image = test_image.unsqueeze(0)  # add batch dimension\n",
        "\n",
        "# Model prediction\n",
        "with torch.no_grad():\n",
        "    prediction = model(test_image)\n",
        "    prediction = torch.sigmoid(prediction)  # Convert logits to probabilities\n",
        "\n",
        "class_names = ['normal', 'abnormal']\n",
        "confidence = torch.sigmoid(prediction).squeeze().item()\n",
        "\n",
        "print(f'This model is {round(100 * confidence, 2)}% confident that the scan is {class_names[1]} and {round(100 * (1 - confidence), 2)}% confident that the scan is {class_names[0]}.')\n",
        "\n",
        "predicted_class = class_names[0] if confidence < 0.5 else class_names[1]\n",
        "\n",
        "print()\n",
        "print(f'Prediction is {predicted_class}.')\n",
        "print(f'{\"Correct ðŸ˜Š\" if predicted_class == class_names[y_val[index].item()] else \"Incorrect ðŸ˜¢\"}')\n",
        "print()\n",
        "\n",
        "plt.axis('off')\n",
        "plt.title(f\"Ans: {'abnormal' if y_val[index] else 'normal'}\")\n",
        "plt.imshow(x_val[index], cmap='gray')  # Assuming the images are grayscale and channel-first\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Takeaways\n",
        "\n",
        "1. **End-to-End Workflow**:\n",
        "   - The tutorial presents a complete workflow of deep learning, from data loading and preprocessing to model construction, training, and evaluation. Each stage, including data normalization, labeling, and the selection of appropriate loss functions for specific tasks, is underscored for its significance.\n",
        "\n",
        "2. **Model Evaluation & Visualization**:\n",
        "   - Beyond just training, the tutorial underscores the importance of evaluating the trained model on unseen data and visualizing its performance metrics (like accuracy and loss). Such visualizations are instrumental in discerning model behavior across epochs, ensuring it's learning effectively and isn't succumbing to overfitting or underfitting.\n"
      ],
      "metadata": {
        "id": "kPyUMVBt-I63"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKcsaX0P-QdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}